{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Policy Exploration Notebook\n",
    "\n",
    "This notebook provides an interactive environment to load, visualize, and analyze the behavior of trained Reinforcement Learning agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import yaml\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "from src.fleet_simulator.simpy_delivery_environment import SimpyDeliveryEnvironment\n",
    "from src.graph_routing_engine.astar_optimization_logic import AStarRouting\n",
    "from src.agent_brains.multi_agent_rl_policy import MultiAgentPolicyContainer\n",
    "from src.feature_forge.graph_embedding_features import GraphEmbeddingFeatures\n",
    "\n",
    "# Ensure Ray is initialized\n",
    "if not ray.is_initialized():\n",
    "    ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'conf/rl_agent_params.yaml'\n",
    "ENV_CONFIG_PATH = 'conf/environments/dev.yaml'\n",
    "ROUTING_CONFIG_PATH = 'conf/routing_engine_config.yaml'\n",
    "OSM_PROCESSING_CONFIG_PATH = 'conf/osm_processing_config.yaml'\n",
    "SCENARIO_PATH = 'data_nexus/simulation_scenarios/tehran_fleet_scenario.pkl'\n",
    "RL_CHECKPOINT_DIR = 'rl_model_registry/latest_checkpoint'\n",
    "GNN_MODEL_PATH = 'rl_model_registry/gcn_model.pth'\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    rl_agent_config = yaml.safe_load(f)\n",
    "\n",
    "with open(OSM_PROCESSING_CONFIG_PATH, 'r') as f:\n",
    "    osm_config = yaml.safe_load(f)\n",
    "\n",
    "graph_path = osm_config['graph_serialization']['output_path']\n",
    "graph = nx.read_gml(graph_path)\n",
    "\n",
    "router = AStarRouting(graph, ROUTING_CONFIG_PATH)\n",
    "gnn_embedder = GraphEmbeddingFeatures(CONFIG_PATH, graph_path)\n",
    "gnn_embedder.load_model_weights(GNN_MODEL_PATH)\n",
    "\n",
    "class MockRLlibEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(128,))\n",
    "    def reset(self, seed=None, options=None):\n",
    "        return self.observation_space.sample(), {}\n",
    "    def step(self, action):\n",
    "        return self.observation_space.sample(), 0.1, False, False, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load RL Agent Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLLibPolicyLoader(Algorithm):\n",
    "    def setup(self, config):\n",
    "        super().setup(config)\n",
    "        # The actual policy objects are stored in the `live_policies` of the worker\n",
    "        # For inference, you often directly load and use the `Policy` objects\n",
    "        # However, for a quick hack, we can mock a trainer and restore it.\n",
    "        # A cleaner way involves `rllib.policy.policy.Policy.from_checkpoint`\n",
    "\n",
    "        from ray.rllib.algorithms.ppo import PPOConfig\n",
    "        from ray.tune.registry import register_env\n",
    "        register_env(\"mock_env\", MockRLlibEnv)\n",
    "\n",
    "        mock_config = (\n",
    "            PPOConfig()\n",
    "            .environment(\"mock_env\")\n",
    "            .framework(\"torch\")\n",
    "            .multi_agent(\n",
    "                policies={\n",
    "                    \"driver_policy\": (\n",
    "                        None,\n",
    "                        gym.spaces.Box(low=-np.inf, high=np.inf, shape=rl_agent_config['multi_agent_config']['policies']['driver_policy']['obs_space']),\n",
    "                        gym.spaces.Discrete(rl_agent_config['multi_agent_config']['policies']['driver_policy']['action_space'][0]),\n",
    "                        {}\n",
    "                    ),\n",
    "                    \"fleet_manager_policy\": (\n",
    "                        None,\n",
    "                        gym.spaces.Box(low=-np.inf, high=np.inf, shape=rl_agent_config['multi_agent_config']['policies']['fleet_manager_policy']['obs_space']),\n",
    "                        gym.spaces.Discrete(rl_agent_config['multi_agent_config']['policies']['fleet_manager_policy']['action_space'][0]),\n",
    "                        {}\n",
    "                    ),\n",
    "                },\n",
    "                policy_mapping_fn=MultiAgentPolicyContainer.map_agent_to_policy,\n",
    "            )\n",
    "        )\n",
    "        self._trainer_for_restore = mock_config.build()\n",
    "        if os.path.exists(RL_CHECKPOINT_DIR):\n",
    "            self._trainer_for_restore.restore(RL_CHECKPOINT_DIR)\n",
    "            print(f\"RL Agent restored from {RL_CHECKPOINT_DIR}\")\n",
    "        else:\n",
    "            print(f\"Warning: RL Agent checkpoint not found at {RL_CHECKPOINT_DIR}. Using untrained policy.\")\n",
    "\n",
    "    def compute_single_action(self, obs, policy_id, explore):\n",
    "        return self._trainer_for_restore.compute_single_action(obs, policy_id=policy_id, explore=explore)\n",
    "\n",
    "rl_inferer_for_sim = RLLibPolicyLoader({\"env\": \"mock_env\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulate and Analyze Policy Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_env = SimpyDeliveryEnvironment(ENV_CONFIG_PATH, SCENARIO_PATH, router, rl_inferer_for_sim)\n",
    "metrics_df = sim_env.run_simulation(until=1800) # Run for 30 minutes simulated time\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.lineplot(x='time', y='num_delivered_orders', data=metrics_df, label='Delivered')\n",
    "sns.lineplot(x='time', y='num_pending_orders', data=metrics_df, label='Pending')\n",
    "sns.lineplot(x='time', y='num_in_transit_orders', data=metrics_df, label='In Transit')\n",
    "plt.title('Order Status Over Time')\n",
    "plt.xlabel('Simulation Time (s)')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.lineplot(x='time', y='total_driver_distance', data=metrics_df, label='Total Distance')\n",
    "sns.lineplot(x='time', y='total_driver_time', data=metrics_df, label='Total Time')\n",
    "plt.title('Driver Cumulative Metrics')\n",
    "plt.xlabel('Simulation Time (s)')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Simulation Metrics:\")\n",
    "print(metrics_df.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Individual Agent Action Visualization (Conceptual)\n",
    "\n",
    "This section would typically involve sampling observations from the environment and feeding them to the policy to see the predicted actions. It's difficult to visualize without a concrete environment interaction loop here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulate an observation for a driver and get action\n",
    "dummy_driver_obs = np.random.rand(128).astype(np.float32)\n",
    "driver_action = rl_inferer_for_sim.compute_single_action(dummy_driver_obs, policy_id=\"driver_policy\", explore=False)\n",
    "print(f\"Sample Driver Action: {driver_action}\")\n",
    "\n",
    "dummy_dispatcher_obs = np.random.rand(256).astype(np.float32)\n",
    "dispatcher_action = rl_inferer_for_sim.compute_single_action(dummy_dispatcher_obs, policy_id=\"fleet_manager_policy\", explore=False)\n",
    "print(f\"Sample Dispatcher Action: {dispatcher_action}\")\n",
    "\n",
    "# Further analysis would involve mapping actions back to meaningful decisions (e.g., node IDs, route segments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
